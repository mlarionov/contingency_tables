{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularized Saturated Models for Classification Tasks with Categorical Variables\n",
    "\n",
    "Contingency tables are great for probablilty mass estimation, but can also be used for classification and regression tasks. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume we have $M$ independent variables, for which we will use $i$, $j$, $k$ to denote an index. We also have one dependent variable, the values of which is are indexed using $v$. Then the classification problem can be written as :\n",
    "\n",
    "$$ p(y_v | x_{ij\\ldots k}) = \\frac{p_{vij\\ldots k}}{p_{+ij\\ldots k}} $$\n",
    "\n",
    "Here the symbol $+$ implies taking a sum, like in this example:\n",
    "\n",
    "$$ p_{i+} = \\sum_k{p_{ik}} $$\n",
    "\n",
    "So if we have the probability mass problem solved it is trivial to solve the classification problem as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To estimate $p_{vij\\ldots k}$, we take the counts of the training data $m_{vij\\ldots k}$ and use a Multinomial model with the following log-likelihood:\n",
    "\n",
    "$$ L = \\sum_{vij\\ldots k}{m_{vij\\ldots k}log p_{vij\\ldots k}}$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes prior\n",
    "\n",
    "The likelihood function above is the same as for the probability mass estimate, but the prior is slightly different. Here our prior assumptions are that the data can be classified using Naive Bayes algorithm and a weaker assumption that all the variables are independent (we need that weaker assumption to make sure none of the probabilities $p_{vij\\ldots k}$ are zero.\n",
    "\n",
    "The Naive Bayes assumption can be expressed like this:\n",
    "\n",
    "$$ p_{vij\\ldots k} = p(y, x_1\\ldots x_M) = p(y) \\prod_{m=1}^M{p(x_m|y)} = \\big[p(y)\\big]^{1-M} \\prod_{m=1}^M{p(x_m,y)} $$\n",
    "\n",
    "Here $p(x_m|y)$ is derived by summing along all indexes except y and m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Independence prior\n",
    "\n",
    "Independence prior is also very simple:\n",
    "\n",
    "$$ p_{vij\\ldots k} = p(y) \\prod_{m=1}^M{p(x_m)}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posterior distribution\n",
    "\n",
    "Here we will bring together likelihood and both prior distributions.\n",
    "\n",
    "Let:\n",
    "\n",
    "$$ R_{vij\\ldots k} = \\lambda_1\\big(\\big[p(y)\\big]^{1-M} \\prod_{m=1}^M{p(x_m,y)}\\big) + \\lambda_2 \\big( p(y) \\prod_{m=1}^M{p(x_m)}\\big), $$\n",
    "\n",
    "where $\\lambda_1$ and $\\lambda_2$ are hyperparameters.\n",
    "\n",
    "Then the log of the posterior distribution is:\n",
    "\n",
    "$$ \\mathcal{P} = \\sum_{vij\\ldots k}({m_{vij\\ldots k} + R_{vij\\ldots k})log p_{vij\\ldots k}} - \\sum_{ij}{log \\Gamma(R_{vij\\ldots k})}$$\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mercedes-Benz data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import LabelEncoder,KBinsDiscretizer\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.read_csv('data/train.csv.zip')\n",
    "kaggle_test_data = pd.read_csv('data/test.csv.zip')\n",
    "combined = pd.concat([all_data, kaggle_test_data], axis=0, sort=False)\n",
    "cat_column_names = ['X0', 'X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X8']\n",
    "label_encoders = {col: LabelEncoder().fit(combined[col]) for col in cat_column_names}\n",
    "\n",
    "def preprocess_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    # Removing duplicate or constant columns\n",
    "    # as per https://www.kaggle.com/yohanb/categorical-features-encoding-xgb-0-554\n",
    "    columns_to_remove = ['X93', 'X107', 'X233', 'X235', 'X268', 'X289', 'X290', 'X293', 'X297', 'X330', 'X347',\n",
    "                     'X382', 'X232', 'X279', 'X35', 'X37', 'X39', 'X302', 'X113', 'X134', 'X147', 'X222',\n",
    "                     'X102', 'X214', 'X239', 'X76', 'X324', 'X248', 'X253', 'X385', 'X172', 'X216', 'X213',\n",
    "                     'X84', 'X244', 'X122', 'X243', 'X320', 'X245', 'X94', 'X242', 'X199', 'X119', 'X227',\n",
    "                     'X146', 'X226', 'X326', 'X360', 'X262', 'X266', 'X247', 'X254', 'X364', 'X365', 'X296', 'X299',\n",
    "                     'X11', 'X93', 'X107', 'X233', 'X235', 'X268', 'X289', 'X290', 'X293', 'X297', 'X330', 'X347']\n",
    "    data1 = df.drop(columns=columns_to_remove)\n",
    "    for col, encoder in label_encoders.items():\n",
    "        data1[col] = encoder.transform(data1[col])\n",
    "        \n",
    "    return data1\n",
    "\n",
    "train_data = preprocess_data(all_data)\n",
    "kaggle_data = preprocess_data(kaggle_test_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discretizing the target variable\n",
    "\n",
    "In this task the target variable y is actually continuous. Here  is the histogram of this variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data.iloc[:,1].hist(bins=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a multimodal distribution with a certain degree of smoothness. To change it to a continuous variable we can discretize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_bins = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This really should be done on the training set.\n",
    "# We will do that once we make sure the model is working\n",
    "discretizer = KBinsDiscretizer(n_bins=y_bins, encode='ordinal', )\n",
    "discretized_y = discretizer.fit_transform(train_data.y.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function does inverse transform, given an array of probabilities\n",
    "def inverse_discretizer(discretizer, probabilities):\n",
    "    assert discretizer.n_bins_ == probabilities.shape[-1]\n",
    "    all_options = np.arange(discretizer.n_bins_).reshape(-1,1)\n",
    "    all_options_transformed = discretizer.inverse_transform(all_options)\n",
    "    expected_y = probabilities @ all_options_transformed\n",
    "    return expected_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_inverse_discretizer():\n",
    "    discretizer = KBinsDiscretizer(n_bins=50, encode='ordinal', )\n",
    "    discretized_y = discretizer.fit_transform(train_data.y.values.reshape(-1, 1))\n",
    "    probabilities = np.zeros([5, 50])\n",
    "    probabilities[:,-1] = 1\n",
    "    print(inverse_discretizer(discretizer, probabilities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[197.3096]\n",
      " [197.3096]\n",
      " [197.3096]\n",
      " [197.3096]\n",
      " [197.3096]]\n"
     ]
    }
   ],
   "source": [
    "test_inverse_discretizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the deep learning model\n",
    "\n",
    "First we need to figure out the shape of the parameter array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the shape of the parameter array\n",
    "def get_parameter_shape(input_array):\n",
    "    return tuple(np.max(input_array, axis=0).astype(int)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "319"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_shape = get_parameter_shape(train_data.iloc[:,2:])\n",
    "len(test_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[49., 37., 23., ...,  0.,  0.,  0.],\n",
       "       [ 6., 37., 21., ...,  0.,  0.,  0.],\n",
       "       [ 0., 24., 24., ...,  0.,  0.,  0.],\n",
       "       [ 1., 24., 21., ...,  0.,  0.,  0.],\n",
       "       [ 1., 24., 23., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discretized_train_data = np.hstack([discretized_y, train_data.values[:,2:]])\n",
    "discretized_train_data[:5,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we cannot use the multidimensional array with this many dimensions, we have to flatten it. For this we need to create a map from the array of features to a position in the probability array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dimensions():\n",
    "    \"To get the dimensionality of all dimensions\"\n",
    "    columns = list(train_data.columns)[2:]\n",
    "    dimensions = [50]\n",
    "    for column  in columns:\n",
    "        if column in label_encoders.keys():\n",
    "            dimensions.append(len(label_encoders[column].classes_))\n",
    "        else:\n",
    "            dimensions.append(2)\n",
    "    return dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions = get_dimensions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(data, lambda_1, lambda_2, epochs, verbose=False):\n",
    "    \"\"\"\n",
    "    data: assuming the first column is discretized y!\n",
    "    \"\"\"\n",
    "    shape = get_parameter_shape(data)\n",
    "    print(shape)\n",
    "    data = data.astype(int)\n",
    "      \n",
    "    c_init = tf.zeros_initializer()\n",
    "    c = tf.Variable(initial_value=c_init(shape=shape, dtype=\"float32\"), trainable=True)\n",
    "    \n",
    "    def _get_probabilities():\n",
    "        c_flattened = tf.reshape(c, (1,-1))\n",
    "        p = tf.reshape(tf.nn.softmax(c_flattened), shape)\n",
    "        log_p = tf.reshape(tf.nn.log_softmax(c_flattened), shape)\n",
    "        return p, log_p\n",
    "    \n",
    "    def compute_loss():\n",
    "        epsilon = 1E-10\n",
    "        p, log_p = _get_probabilities()\n",
    "        \n",
    "        #Marginal probabilities\n",
    "        all_dimensions = list(range(len(shape)))\n",
    "        p1 = [tf.reduce_sum(p, all_dimensions-[i], keepdims=True) for i in all_dimensions]\n",
    "        if verbose:\n",
    "            print('p1[5].shape', p1[5].shape)\n",
    "        marginal_p = functools.reduce(tf.multiply, p1)\n",
    "        \n",
    "        #Probabilities of naive-bayes\n",
    "        x_dimensions = list(range(1,len(shape)))\n",
    "        p2 = [tf.reduce_sum(p, x_dimensions-[i], keepdims=True) for i in x_dimensions]\n",
    "        if verbose:\n",
    "            print('p2[5].shape', p2[5].shape)\n",
    "        naive_bayes = functools.reduce(tf.multiply, p2)\n",
    "        naive_bayes = p1[0] ** (-len(x_dimensions)) * naive_bayes\n",
    "        \n",
    "        log_likelihood_tensor = tf.gather_nd(log_p, data)\n",
    "        if verbose:\n",
    "            print('log_likelihood_tensor.shape', log_likelihood_tensor.shape)           \n",
    "        log_likelihood = tf.reduce_mean(log_likelihood_tensor, axis=None)\n",
    "        \n",
    "        log_prior_central_tendency = lambda_1 * naive_bayes + lambda_2 * marginal_p\n",
    "        log_prior_tensor = log_prior_central_tendency * log_p + \\\n",
    "            tf.math.lgamma(log_prior_central_tendency + epsilon)\n",
    "        if verbose:\n",
    "            print('log_prior_tensor.shape', log_prior_tensor.shape)\n",
    "        log_prior = tf.reduce_mean(log_prior_tensor, axis=None)\n",
    "        \n",
    "        return -log_likelihood-log_prior\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=3.0)\n",
    "    for i in range(epochs):\n",
    "        optimizer.minimize(compute_loss, var_list=[c])\n",
    "        if (i % 20 == 0) and verbose:\n",
    "            print(compute_loss().numpy())\n",
    "    \n",
    "    \n",
    "    return _get_probabilities()[0].numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the quick rundown of the code:\n",
    "1. The probabilities $p_{ij}$ take the values between 0 and 1 and should all add up to 1. For this reason I used a common trick and use `softmax` function that takes a variable $c$ and produces $p$ that satisfies these constraints. Correspondingly, the model learns the values of $c$.\n",
    "1. TensorFlow also provides function `log_softmax` that helps avoid the case of extreme large values returned by `softmax`\n",
    "1. The code is using TensorFlow 2.x. So instead of using `Session.run()` we just call a function (in our case `_get_probabilities()`). Honestly, I don't miss TensorFlow 1.x\n",
    "1. To compute $p_{i+} p_{+j}$ we use matrix multiplication. It is the fastest way.\n",
    "1. Function `lgamma()` is used instead of computing Gamma function first then taking a log\n",
    "1. Gamma function of zero is not defined. This makes the cost function unstable. To deal with that we add a small positive value `epsilon` to the argument, which takes care of the issue.\n",
    "1. We have a custom training loop and using Adam optimizer. \n",
    "1. The function returns an Numpy array of probabilities.\n",
    "\n",
    "Let's train the model for several values of the regularization parameter $\\lambda$:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = train_model(discretized_train_data, 0.5, 0.5, 200, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = to_data_frame(result)\n",
    "probabilities* np.sum(salary_experience_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, the predicted counts match the training data very closely. Let's see what the predictions would be for high value of regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = run_experiment(salary_experience_array, 500, 100, False)\n",
    "metric(salary_experience_array, result * np.sum(salary_experience_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = to_data_frame(result)\n",
    "probabilities* np.sum(salary_experience_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the dependence is much lower, and the predictions is much more smooth, meaning that there are no zeros or exreme values. However the model does not fit very well to the data. How to find optimal regularization parameter $\\lambda$? We will use cross-validation, a common method in machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation\n",
    "Our plan is to generate five folds of data and run the train the model for different values of the regularization hyperparameter. For every split we train the model on the training data and test it on the test data. As a metric here we use cross-entropy loss (the first component returning by the `result_metric` function). We use grid search in logarithmic scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_single(data, test_fraction):\n",
    "    \"\"\"Create train and test set\"\"\"\n",
    "    test = np.zeros(shape=data.shape)\n",
    "    for i in range(test.shape[0]):\n",
    "        for j in range(test.shape[1]):\n",
    "            test[i,j] = np.sum(np.random.choice([0,1], size=data[i,j], p=[1-test_fraction, test_fraction]))\n",
    "    return data - test, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating 5 folds\n",
    "np.random.seed(2128506)\n",
    "splits = [sample_single(salary_experience_array, 0.2) for _ in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cv(data, regularization, epochs, verbose=False):\n",
    "    cs_sum = 0\n",
    "    for train, test in splits:\n",
    "        result = run_experiment(train, regularization, epochs, verbose)\n",
    "        result_metric = metric(test, result * np.sum(test))\n",
    "        cs_sum += result_metric['Fit to data']\n",
    "    return cs_sum / len(data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = {regularization: run_cv(salary_experience_array, regularization, 300) \n",
    "              for regularization in [0,2,4,8,16,64,128, 256]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(cv_results, index=['Regularization', \"CV Performance\"])\n",
    "results_df.T.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the cross-validation results the best performance of the model happens when $\\lambda = 64$. We can also see that the model performance changes little after $\\lambda = 4$. Perhaps it makes sense to average several models of comparable performance. Let's see the results generated by the best model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = run_experiment(salary_experience_array, 64, 500, False)\n",
    "metric(salary_experience_array, result * np.sum(salary_experience_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = to_data_frame(result)\n",
    "probabilities* np.sum(salary_experience_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a good balance between prediction smoothness and how well it fit the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Here we considered a fully Bayesian approach for contingency table smoothing. This approach is different from pseudo-Bayes approach [2,3], where the prior distribution is defined using the data. We can take advantage of compute power that was not available in the 1970s, and use the true Bayesian approach to get more consistent results. It is certainly possible to use the same method for a higher number of dimensions as most real world data are. In this article I used a two dimensional contingency table solely because it is much easier to visualize than multidimensional tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] Alan Agresti, Categorical data analysis, Whiley, 1990\n",
    "\n",
    "[2] Yvonne Bishop et al., Discrete Multivariate Analysis, MIT, 1975\n",
    "\n",
    "[3] Stephen E. Fienberg and Paul W. Holland, Simultaneous Estimation of Multinomial\n",
    "Cell Probabilities, Journal of the American Statistical Association 68(343):683-691, 1973"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}